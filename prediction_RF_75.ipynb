{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxHnXy1MtRxX",
    "outputId": "3c3785e5-fda0-434e-a4b2-e8b227995852"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn tqdm imbalanced-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "data = pd.read_csv(r'/content/cleaned_tweets (3).csv')\n",
    "print(f\"Data loaded: {len(data)} tweets\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18bTJy4RBOdO",
    "outputId": "bd4efe9f-69e8-4e81-cbc2-972aa1511d11"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading data...\n",
      "Data loaded: 14640 tweets\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Enhanced text cleaning\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "print(\"Cleaning text...\")\n",
    "data['final_text'] = data['text'].apply(clean_text)"
   ],
   "metadata": {
    "id": "NSDDXykOt9kR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e308fe69-156d-47e8-9f8b-65f17df95a5c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cleaning text...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Enhanced Crisis Detection using both keywords and sentiment\n",
    "def enhanced_crisis_detection(tweet, sentiment):\n",
    "    \"\"\"\n",
    "    Enhanced crisis detection combining keywords and original sentiment\n",
    "    \"\"\"\n",
    "    high_crisis_keywords = [\n",
    "        'canceled', 'cancelled', 'stranded', 'emergency', 'disaster',\n",
    "        'horrible', 'terrible', 'worst', 'chaos', 'nightmare', 'awful',\n",
    "        'refund', 'compensation', 'lost luggage', 'missed connection',\n",
    "        'hours delayed', 'never again', 'disgusting', 'furious'\n",
    "    ]\n",
    "\n",
    "    medium_crisis_keywords = [\n",
    "        'delayed', 'late', 'poor service', 'bad experience',\n",
    "        'unhappy', 'disappointed', 'frustrated', 'annoyed'\n",
    "    ]\n",
    "\n",
    "    tweet_lower = tweet.lower()\n",
    "\n",
    "    # Check for high crisis indicators\n",
    "    high_crisis_count = sum(1 for keyword in high_crisis_keywords if keyword in tweet_lower)\n",
    "    medium_crisis_count = sum(1 for keyword in medium_crisis_keywords if keyword in tweet_lower)\n",
    "\n",
    "    # Crisis scoring system\n",
    "    crisis_score = 0\n",
    "\n",
    "    # Keyword scoring\n",
    "    crisis_score += high_crisis_count * 3\n",
    "    crisis_score += medium_crisis_count * 1\n",
    "\n",
    "    # Sentiment scoring\n",
    "    if sentiment == 'negative':\n",
    "        crisis_score += 2\n",
    "    elif sentiment == 'neutral':\n",
    "        crisis_score += 0.5\n",
    "\n",
    "    # Classification thresholds\n",
    "    if crisis_score >= 4:\n",
    "        return 'high_crisis'\n",
    "    elif crisis_score >= 2:\n",
    "        return 'medium_crisis'\n",
    "    else:\n",
    "        return 'non_crisis'\n"
   ],
   "metadata": {
    "id": "bDD4f1lHa-xA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply enhanced crisis detection\n",
    "print(\"Detecting crisis levels...\")\n",
    "data['crisis_level'] = data.apply(\n",
    "    lambda row: enhanced_crisis_detection(row['final_text'], row['airline_sentiment']),\n",
    "    axis=1\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mO6y_aF-bAeU",
    "outputId": "18923c2c-0dcf-47ed-fb72-35576b4239c9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Detecting crisis levels...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Convert to binary for initial model\n",
    "def crisis_to_binary(crisis_level):\n",
    "    if crisis_level in ['high_crisis', 'medium_crisis']:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "data['crisis_binary'] = data['crisis_level'].apply(crisis_to_binary)\n"
   ],
   "metadata": {
    "id": "WbPtvhRlbGBy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Check class distribution\n",
    "print(\"Crisis Level Distribution:\")\n",
    "print(data['crisis_level'].value_counts())\n",
    "print(f\"\\nBinary Crisis Distribution:\")\n",
    "print(data['crisis_binary'].value_counts())\n",
    "print(f\"Crisis Percentage: {(data['crisis_binary'].sum() / len(data)) * 100:.2f}%\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqgYGBMsbItr",
    "outputId": "1e9f4a25-669f-46e3-fded-5644ab8dcfd5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Crisis Level Distribution:\n",
      "crisis_level\n",
      "medium_crisis    7753\n",
      "non_crisis       5264\n",
      "high_crisis      1623\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Binary Crisis Distribution:\n",
      "crisis_binary\n",
      "1    9376\n",
      "0    5264\n",
      "Name: count, dtype: int64\n",
      "Crisis Percentage: 64.04%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# FAST Feature Engineering - Reduced parameters for speed\n",
    "print(\"Creating features...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,      # Reduced for speed\n",
    "    ngram_range=(1, 2),     # Bigrams only\n",
    "    min_df=5,              # Higher threshold\n",
    "    max_df=0.9,            # Remove very common words\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(data['final_text']).toarray()\n",
    "y = data['crisis_binary'].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GkmM3M5bK_r",
    "outputId": "d98710e6-9c99-466a-f01c-4fd861caf12d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating features...\n",
      "Feature matrix shape: (14640, 2000)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Check for class imbalance\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(f\"Class Distribution: {dict(zip(unique, counts))}\")\n",
    "\n",
    "# Split data\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iR3dDlE8bOLv",
    "outputId": "85932990-39b7-40c1-9f2f-881cb8f5ffff"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Class Distribution: {np.int64(0): np.int64(5264), np.int64(1): np.int64(9376)}\n",
      "Splitting data...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "print(\"Balancing data with SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"After SMOTE - Training set balance:\")\n",
    "unique, counts = np.unique(y_train_balanced, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2_JRkoDbQ3j",
    "outputId": "b14b2167-0a57-48d1-86b6-a960dedb9bb9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Balancing data with SMOTE...\n",
      "After SMOTE - Training set balance:\n",
      "{np.int64(0): np.int64(6563), np.int64(1): np.int64(6563)}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# FASTEST VERSION - Skip GridSearch entirely!\n",
    "print(\"Training Random Forest with optimized fixed parameters...\")\n",
    "\n",
    "best_rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,        # Good balance of speed/accuracy\n",
    "    max_depth=20,           # Prevents overfitting\n",
    "    min_samples_split=5,    # Good default\n",
    "    min_samples_leaf=2,     # Prevents overfitting\n",
    "    max_features='sqrt',    # Fastest option\n",
    "    class_weight='balanced', # Handle imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1,              # Use all CPU cores\n",
    "    verbose=1               # Show progress\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qW19n5ybUVA",
    "outputId": "91fd93bd-db91-422a-a3f3-9eb76efc1f35"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Random Forest with optimized fixed parameters...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Fast direct training\n",
    "best_rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "print(\"Model training completed!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Juublt0KbXWg",
    "outputId": "05afbfd9-09cd-4d02-8b07-4116aac2a8b7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   25.4s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model training completed!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   26.2s finished\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Enhanced evaluation function\n",
    "def evaluate_model(model, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Enhanced model evaluation with multiple metrics\"\"\"\n",
    "\n",
    "    for X_set, y_set, name in [(X_val, y_val, \"Validation\"), (X_test, y_test, \"Test\")]:\n",
    "        preds = model.predict(X_set)\n",
    "        proba = model.predict_proba(X_set)[:, 1]\n",
    "\n",
    "        print(f\"\\n{name} Set Results:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Accuracy: {accuracy_score(y_set, preds):.4f}\")\n",
    "        print(f\"ROC-AUC: {roc_auc_score(y_set, proba):.4f}\")\n",
    "\n",
    "        # Detailed classification report\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(y_set, preds, average=None)\n",
    "        print(f\"\\nPer-class metrics:\")\n",
    "        print(f\"Non-Crisis - Precision: {precision[0]:.4f}, Recall: {recall[0]:.4f}, F1: {f1[0]:.4f}\")\n",
    "        print(f\"Crisis - Precision: {precision[1]:.4f}, Recall: {recall[1]:.4f}, F1: {f1[1]:.4f}\")\n",
    "\n",
    "        print(f\"\\nClassification Report:\")\n",
    "        print(classification_report(y_set, preds, target_names=['Non-Crisis', 'Crisis']))\n"
   ],
   "metadata": {
    "id": "TwUSuvpQbZSi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "evaluate_model(best_rf_model, X_val, y_val, X_test, y_test)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQ5WslESpRmm",
    "outputId": "fd1459e4-6c18-4349-8cae-7d441d5e1d05"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation Set Results:\n",
      "----------------------------------------\n",
      "Accuracy: 0.7368\n",
      "ROC-AUC: 0.8515\n",
      "\n",
      "Per-class metrics:\n",
      "Non-Crisis - Precision: 0.5955, Recall: 0.8340, F1: 0.6948\n",
      "Crisis - Precision: 0.8799, Recall: 0.6823, F1: 0.7686\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Crisis       0.60      0.83      0.69       789\n",
      "      Crisis       0.88      0.68      0.77      1407\n",
      "\n",
      "    accuracy                           0.74      2196\n",
      "   macro avg       0.74      0.76      0.73      2196\n",
      "weighted avg       0.78      0.74      0.74      2196\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test Set Results:\n",
      "----------------------------------------\n",
      "Accuracy: 0.7464\n",
      "ROC-AUC: 0.8528\n",
      "\n",
      "Per-class metrics:\n",
      "Non-Crisis - Precision: 0.6041, Recall: 0.8557, F1: 0.7082\n",
      "Crisis - Precision: 0.8942, Recall: 0.6849, F1: 0.7757\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Crisis       0.60      0.86      0.71       790\n",
      "      Crisis       0.89      0.68      0.78      1406\n",
      "\n",
      "    accuracy                           0.75      2196\n",
      "   macro avg       0.75      0.77      0.74      2196\n",
      "weighted avg       0.79      0.75      0.75      2196\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Feature importance analysis\n",
    "print(\"Analyzing feature importance...\")\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_importance = best_rf_model.feature_importances_\n",
    "top_indices = np.argsort(feature_importance)[-20:][::-1]\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "print(\"-\" * 40)\n",
    "for i, idx in enumerate(top_indices):\n",
    "    print(f\"{i+1:2d}. {feature_names[idx]:20s} - {feature_importance[idx]:.4f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3aJScjUpRph",
    "outputId": "b4d033e2-2cad-4fe5-ac1c-8e40e2176d4b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Analyzing feature importance...\n",
      "\n",
      "Top 20 Most Important Features:\n",
      "----------------------------------------\n",
      " 1. cancelled            - 0.0663\n",
      " 2. thank                - 0.0473\n",
      " 3. flight               - 0.0473\n",
      " 4. thanks               - 0.0441\n",
      " 5. hours                - 0.0427\n",
      " 6. hold                 - 0.0353\n",
      " 7. cancelled flightled  - 0.0254\n",
      " 8. delayed              - 0.0230\n",
      " 9. hour                 - 0.0222\n",
      "10. flightled            - 0.0218\n",
      "11. great                - 0.0196\n",
      "12. hrs                  - 0.0174\n",
      "13. plane                - 0.0150\n",
      "14. worst                - 0.0149\n",
      "15. phone                - 0.0146\n",
      "16. service              - 0.0133\n",
      "17. waiting              - 0.0132\n",
      "18. customer             - 0.0130\n",
      "19. dont                 - 0.0124\n",
      "20. delay                - 0.0120\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Enhanced prediction function\n",
    "def predict_crisis_with_confidence(tweets, model, vectorizer):\n",
    "    \"\"\"\n",
    "    Predict crisis with confidence scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaned_tweets = [clean_text(tweet) for tweet in tweets]\n",
    "        features = vectorizer.transform(cleaned_tweets).toarray()\n",
    "\n",
    "        predictions = model.predict(features)\n",
    "        probabilities = model.predict_proba(features)\n",
    "\n",
    "        results = []\n",
    "        for tweet, pred, prob in zip(tweets, predictions, probabilities):\n",
    "            crisis_prob = prob[1]\n",
    "            confidence = max(prob)\n",
    "\n",
    "            result = {\n",
    "                'tweet': tweet,\n",
    "                'prediction': 'Crisis' if pred == 1 else 'Non-Crisis',\n",
    "                'crisis_probability': crisis_prob,\n",
    "                'confidence': confidence,\n",
    "                'severity': 'High' if crisis_prob > 0.8 else 'Medium' if crisis_prob > 0.5 else 'Low'\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {e}\")\n",
    "        return []\n"
   ],
   "metadata": {
    "id": "7WKtjdI8pRtR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Test with example tweets\n",
    "example_tweets = [\n",
    "    \"Flight canceled by Delta, I'm stranded at the airport for 8 hours!\",\n",
    "    \"Slight delay with United, but crew was helpful\",\n",
    "    \"Great experience with Southwest, smooth flight\",\n",
    "    \"Emergency landing due to technical failure, terrible experience\",\n",
    "    \"Lost my luggage and no one can help me, this is a nightmare!\",\n",
    "    \"Food was okay, nothing special but acceptable service\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CRISIS PREDICTION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    predictions = predict_crisis_with_confidence(example_tweets, best_rf_model, vectorizer)\n",
    "\n",
    "    if predictions:  # Check if we got results\n",
    "        for result in predictions:\n",
    "            print(f\"\\nTweet: {result['tweet']}\")\n",
    "            print(f\"Prediction: {result['prediction']}\")\n",
    "            print(f\"Crisis Probability: {result['crisis_probability']:.3f}\")\n",
    "            print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "            print(f\"Severity Level: {result['severity']}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No predictions generated. Check for errors above.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")\n",
    "    print(\"Let's try a simpler prediction method...\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rn_OpqEgpRwE",
    "outputId": "177e48ba-4b50-4f9c-c019-6ae90a1b2b1e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "CRISIS PREDICTION RESULTS\n",
      "============================================================\n",
      "\n",
      "Tweet: Flight canceled by Delta, I'm stranded at the airport for 8 hours!\n",
      "Prediction: Crisis\n",
      "Crisis Probability: 0.680\n",
      "Confidence: 0.680\n",
      "Severity Level: Medium\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet: Slight delay with United, but crew was helpful\n",
      "Prediction: Crisis\n",
      "Crisis Probability: 0.556\n",
      "Confidence: 0.556\n",
      "Severity Level: Medium\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet: Great experience with Southwest, smooth flight\n",
      "Prediction: Non-Crisis\n",
      "Crisis Probability: 0.385\n",
      "Confidence: 0.615\n",
      "Severity Level: Low\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet: Emergency landing due to technical failure, terrible experience\n",
      "Prediction: Crisis\n",
      "Crisis Probability: 0.512\n",
      "Confidence: 0.512\n",
      "Severity Level: Medium\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet: Lost my luggage and no one can help me, this is a nightmare!\n",
      "Prediction: Crisis\n",
      "Crisis Probability: 0.632\n",
      "Confidence: 0.632\n",
      "Severity Level: Medium\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet: Food was okay, nothing special but acceptable service\n",
      "Prediction: Non-Crisis\n",
      "Crisis Probability: 0.477\n",
      "Confidence: 0.523\n",
      "Severity Level: Low\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "    # Simple fallback prediction\n",
    "    for tweet in example_tweets:\n",
    "        cleaned = clean_text(tweet)\n",
    "        features = vectorizer.transform([cleaned]).toarray()\n",
    "        pred = best_rf_model.predict(features)[0]\n",
    "        prob = best_rf_model.predict_proba(features)[0]\n",
    "\n",
    "        print(f\"\\nTweet: {tweet}\")\n",
    "        print(f\"Prediction: {'Crisis' if pred == 1 else 'Non-Crisis'}\")\n",
    "        print(f\"Crisis Probability: {prob[1]:.3f}\")\n",
    "        print(\"-\" * 50)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEkwTq_XpRyp",
    "outputId": "02e084c9-ec26-4e42-a342-c1596f59f33e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Tweet: Flight canceled by Delta, I'm stranded at the airport for 8 hours!\n",
      "Prediction: Crisis\n",
      "Crisis Probability: 0.680\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet: Slight delay with United, but crew was helpful\n",
      "Prediction: Crisis\n",
      "Crisis Probability: 0.556\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Tweet: Great experience with Southwest, smooth flight\n",
      "Prediction: Non-Crisis\n",
      "Crisis Probability: 0.385\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet: Emergency landing due to technical failure, terrible experience\n",
      "Prediction: Crisis\n",
      "Crisis Probability: 0.512\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Tweet: Lost my luggage and no one can help me, this is a nightmare!\n",
      "Prediction: Crisis\n",
      "Crisis Probability: 0.632\n",
      "--------------------------------------------------\n",
      "\n",
      "Tweet: Food was okay, nothing special but acceptable service\n",
      "Prediction: Non-Crisis\n",
      "Crisis Probability: 0.477\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Save the model and vectorizer\n",
    "import joblib\n",
    "print(\"\\nSaving model and vectorizer...\")\n",
    "joblib.dump(best_rf_model, 'crisis_detection_model.pkl')\n",
    "joblib.dump(vectorizer, 'crisis_vectorizer.pkl')\n",
    "print(\"Model and vectorizer saved successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Your crisis detection model is ready to use!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- crisis_detection_model.pkl\")\n",
    "print(\"- crisis_vectorizer.pkl\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBt8fpsQpbjh",
    "outputId": "b12274fe-f8b6-4f5d-ee69-f525a1ab200f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Saving model and vectorizer...\n",
      "Model and vectorizer saved successfully!\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "Your crisis detection model is ready to use!\n",
      "Files saved:\n",
      "- crisis_detection_model.pkl\n",
      "- crisis_vectorizer.pkl\n"
     ]
    }
   ]
  }
 ]
}